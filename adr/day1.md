# Один микросервис - один контейнер. День 1

Этой первая статья о том как идея от прототипа проходит путь до полноценного продукта при участии архитектуры. Оформлена она в виде записей архитектурных решений (ADR - Architecture Decision Records), где каждый этап развития зафиксирован в днях (см. заголовок статьи). 
Все решения будут применяться относительно вымышленного продукта, но рассматриваем реальные проблемы с которыми сталкиваются архитекторы и команды разработки. Документация и код доступны в репозитории [github](https://github.com/arootcom/grpc-fastapi). 

## Контекст

Разработан [прототип](https://github.com/arootcom/grpc-fastapi/tree/day1) на Python, который состоит из трех gRPC микросервисов (заказы, резервирование и лояльность) и API шлюз с последовательным вызовом методов резервирования товаров, применения лояльности и сохранение данных о заказе. [Приложение](https://github.com/arootcom/grpc-fastapi/blob/main/docker-compose-singl.yaml) запускает связанные микросервисы как отдельные процессы используя библитеку [asyncio](https://docs.python.org/3/library/asyncio.html) асинхронного программирования. 
Все это упаковано в один контейнер. Т.к. микросервисы независимые и их взаимодействие может вызвать сложности, а ошибки в одном сервисе могут потянуть за собой цепочку проблем в других, то это может привести к проблемам маштабируемости, отказоустойчивости, мониторинге и безопастности. 

![Component](./day1/deploy-singl.svg)

Что бы запустить прототип необходимо в первую очередь собрать образ контейнера, для этого выполните следущие действия:

1. Склонировать репозиторий проекта

```
$ git clone git@github.com:arootcom/grpc-fastapi.git
$ cd ./grpc-fastapi
```

2. Переключиться на ветку day1

```
$ git remote update
$ git checkout -b day1 origin/day1
$ git pull
```

Если все выполнено верно, то при просмотре списка локальных веток будет следущий результат, где рабочая ветка будет помечена знаком *

```
$ git branch
* day1
  main
```

2. Собрать образ контейнера pyton3-grpc

```
$ docker build -t python3-grpc .
```

Список доступных, локально собранных, образов должен обязательно содержать две записи

```
$ docker images
REPOSITORY     TAG       IMAGE ID       CREATED        SIZE
python3-grpc   latest    1c1383197816   45 hours ago   1.25GB
python         3         4db51a7249e1   2 weeks ago    1.12GB
```

## Решение

Каждый микросервис оформить в отдельный контейнер. Это позволит изолировать сервис, обеспечить независимость в развертывании и маштабировании.

1. Минимизация размера. Контейнер содержит только необходимые зависимости и код, что упрощает управление ресурсами.
2. Эффективное масштабирование. Каждый микросервис может быть масштабирован независимо от других, обеспечивая оптимальное распределение нагрузки.
3. Упрощённое обновление и управление. Изменения в одном микросервисе не требуют пересоздания всех контейнеров.
4. Гибкое управление зависимостями и версиями. Различные версии микросервисов могут существовать параллельно, не влияя на работу других сервисов.

## Обоснование

Принцип «Один микросервис — один контейнер» в микросервисной архитектуре обоснован необходимостью изоляции и переносимости микросервисов. Этот подход позволяет разрабатывать, тестировать и масштабировать компоненты по отдельности, а также обеспечивать согласованность между средой разработки, тестовой средой и производственной средой.

- Изоляция — контейнеры «не мешают» друг другу.
- Переносимость — контейнеризированное приложение можно запускать в различных типах инфраструктуры — на «голом железе», в виртуальных машинах и в облаке — без необходимости его рефакторинга для каждой среды.
- Экономия ресурсов — контейнеры используют ядро хостовой операционной системы без полной виртуализации оборудования, что делает их легковесными и даёт экономию ресурсов по сравнению с традиционными виртуальными машинами.

## Последствия

Когда микросервисов много, управлять контейнерами вручную становится затруднительно — здесь на помощь приходят оркестраторы — системы автоматизации развёртывания, масштабирования, балансировки и мониторинга контейнеров. Например, Kubernetes обеспечивает запуск контейнеров, их мониторинг, автоматический запуск после сбоев и масштабирование сервисов в зависимости от нагрузки.

Преимущества

- Простота поддержки и обновления — команды могут работать над разными сервисами параллельно, не мешая друг другу.
- Возможность масштабировать отдельно нужные части системы — например, если один компонент испытывает высокую нагрузку, можно увеличить ресурсы только для него, а не для всего приложения.
- Легкость модернизации — старые сервисы можно постепенно заменять новыми без полного переписывания кода

Недостатки

- Сложность тестирования — сначала нужно отдельно тестировать каждый микросервис, а потом совместное взаимодействие его с другими элементами системы.
